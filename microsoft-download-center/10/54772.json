{
    "error": "",
    "downloadTitle": "HPC Pack 2012 R2 Update 3 Fixes",
    "downloadDescription": "This update fixes SOA, Management and Scheduler issues of HPC Pack 2012 R2 Update 3.",
    "downloadFile": [
        {
            "isPrimary": "False",
            "name": "KB3189996-x64.exe",
            "url": "https://download.microsoft.com/download/3/8/d/38d15736-3f75-4345-a842-7d0e8780d204/KB3189996-x64.exe",
            "size": "65560992",
            "version": "4.05.5161",
            "datePublished": "7/15/2024 4:46:49 AM"
        },
        {
            "isPrimary": "False",
            "name": "KB3189996-x86.exe",
            "url": "https://download.microsoft.com/download/3/8/d/38d15736-3f75-4345-a842-7d0e8780d204/KB3189996-x86.exe",
            "size": "65491360",
            "version": "4.05.5161",
            "datePublished": "7/15/2024 4:46:49 AM"
        },
        {
            "isPrimary": "False",
            "name": "HpcWebComponents.msi",
            "url": "https://download.microsoft.com/download/3/8/d/38d15736-3f75-4345-a842-7d0e8780d204/HpcWebComponents.msi",
            "size": "1708032",
            "version": "4.05.5161",
            "datePublished": "7/15/2024 4:46:49 AM"
        }
    ],
    "localeDropdown": [
        {
            "cultureCode": "en-us",
            "name": "English"
        },
        {
            "cultureCode": "zh-cn",
            "name": "Chinese (Simplified)"
        },
        {
            "cultureCode": "ja-jp",
            "name": "Japanese"
        }
    ],
    "detailsSection": "This update fixes some known issues of HPC Pack 2012 R2 Update 3: <br/>\r\n<br/>\r\n<b>SOA fixes</b><br/>\r\n- Removed 4 MB message size limit - Now in SOA requests you can send requests that are larger than 4 MB in size. A large request will be split into smaller messages when persisting in MSMQ, where there is 4MB message size restriction;<br/>\r\n- Configurable broker dispatcher capacity - Users can specify the broker dispatcher capacity instead of the calculated cores. This achieves more accurate grow and shrink behavior if the resource type is node or socket. Please refer the sample below: <br/>\r\n&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&lt;loadBalancing dispatcherCapacityInGrowShrink=\"0\"/&gt;<br/>\r\n&nbsp&nbsp&nbspIf value is 0 \u2013 dispatcher capacity is auto calculated by the number of cores. If value is an positive integer, dispatcher capacity will be the value specified.<br/>\r\n&nbsp&nbsp&nbspDispatcher Capacity is defined as the number of requests that a service host can handle at a time, by default it is the number of cores a service host occupies. This value can also be specified by sessionStartInfo.BrokerSettings.DispatcherCapacityInGrowShrink per session<br/>\r\n- An optional parameter \u2018jobPriority\u2019 is added in ExcelClient.OpenSession method for Excel VBA;<br/>\r\n- Added GPU Unit type in SOA session API so that you can specify GPU resource in the SOA job;<br/>\r\n- Fixed an issue that HA broker nodes may not be found by the system due to AccessViolationException in session service;<br/>\r\n- Fixed an issue that SOA job may be stuck in queued state;<br/>\r\n- Reduced the SOA job queue time in Balance/Graceful Preemption mode;<br/>\r\n- Fixed an issue that durable session job may runaway when the client sends requests without flush and then disconnects;<br/>\r\n- Fixed broker worker crash issue in some rare situation;<br/>\r\n- Fixed an issue that a session job may stall when azure worker role nodes get re-deployed in a large deployment;<br/>\r\n- Fixed an issue that SOA request may fail in some rare condition with large azure burst deployment;<br/>\r\n- Added ParentJobIds in SessionStartInfo for SOA Session API so that parent jobs can be specified during session creation;<br/>\r\n- Added ServiceHostIdleTimeout for SOA service, and the default value is 60 minutes;<br/>\r\n<br/>\r\n<b>Scheduler and API fixes </b><br/>\r\n- Fix overflow in AllocationHistory table; This requires SQL Server 2012 or later version;<br/>\r\n- Add cluster property JobCleanUpDayOfWeek to specify on which day of week should HPC Pack clean up Scheduler DB. For example, to let the service do job clean up on every Saturday, admin need to set:<br/>\r\n&nbsp&nbsp&nbsp&nbsp&nbsp&nbspSet-HPCClusterProperty -JobCleanUpDayOfWeek \u201cSaturday\u201d<br/>\r\n- Fix an issue that task may failed with \u201cThe parameter is incorrect\u201d message for both on-premise and azure HPC IaaS cluster<br/>\r\n- Fix a scheduler crash issue during startup;<br/>\r\n- Enabled GPU related metrics;<br/>\r\n- Improved of error handling for linux node manager;<br/>\r\n- Fix a deadlock issue when finishing a job or a task to avoid queuing the whole cluster;<br/>\r\n- Fix an issue that a job stuck in canceling and won\u2019t release resource for other jobs resulting the whole cluster being blocked;<br/>\r\n- Improve performance (added a few SQL index) when there is huge historical data;<br/>\r\n- Added cluster configuration \u201cDisableResourceValidation\u201d. Now admin can set this value to true to skip validation on job resource requirement whether can be met by the current available resource. This allows user to submit jobs to a cluster without resource added or provisioned. To change the setting:<br/>\r\n&nbsp&nbsp&nbsp&nbsp&nbsp&nbspSet-HPCClusterProperty -DisableResourceValidation $newValue<br/>\r\n- Included job modification in job audit events. To see all job modification and activities, please try view the \u201cActivity Log\u201d in the job management UI or the output of command \u201cjob view &lt;jobid&gt; /history\";<br/>\r\n- Added new job action \"Hold\" in job GUI; Now you can hold a running job so that no new resources will be allocated to this job. And the job will be in \u201cDraining\u201d state if there is still active tasks running;<br/>\r\n- Fix an issue that release task may be skipped to run in exclusive job;<br/>\r\n- Fix an issue that clusrun may fail to get output from azure compute nodes due to compute node IP changes under auto grow shrink situation;<br/>\r\n- <b>Task execution filter</b> - Task execution filter for Linux compute nodes to enable calling administrator-customized scripts that each time a task is executed on Linux nodes. This helps to enable scenarios such as executing tasks with an Active Directory account on Linux nodes and mounting a user's home folder for task execution. For more information, check \"Get started with HPC Pack task execution filter\";<br/>\r\n- Set maximum memory for your task to be allowed during execution. User can add environment variable \u2018CCP_MAXIMUMMEMORY\u2019 in task, then the task will be marked as failed if the task tries to exceed the memory limitation set by this value on windows compute node. This setting currently isn't appliable on linux compute node;<br/>\r\n- <b>Task Level Node Group</b>: We added initial support for specifying node group information for your tasks instead of specifying this information at the job level. A few things you need to be aware when using this feature:<br/>\r\n&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp1. You\u2019d better using this feature in Queued Scheduling Mode<br/>\r\n&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp2. You can only assign one requested node group for your task and meanwhile, you shall not specify node groups for your job<br/>\r\n&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp3. It is better you specify node groups without overlapping for your tasks within a job<br/>\r\n&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp4. Currently you can specify the task requested node group in the scheduler API, job GUI or CLI<br/>\r\n<br/>\r\n<b>Management fixes</b> <br/>\r\n- Fix a socket exhaustion issue when AzureStorageConnectionString isn\u2019t correctly configured;<br/>\r\n- Fix an issue that SDM service can consume 100% of CPU time on the headnode some time;<br/>\r\n- Add \u2018Networks\u2019 in return object in \u2018Get-HpcNode\u2019 powershell cmdlet;<br/>\r\n- Support new Azure role size in azure bursting including Av2 and H series<br/>\r\n- Fix an issue that admin may fail to remove an HPC user whose account is already removed from AD;<br/>\r\n- Support GPU on workstation node as well as Linux nodes;<br/>\r\n- Fix one issue for selecting OS version when add Azure batch pool in HPC Pack;<br/>\r\n- Fix one issue that the heatmap sometime showing empty;<br/>\r\n- Improve Auto grow shrink script, make the node online first before growing instead of waiting all nodes in OK state. Now the node will be able to accept jobs once it is started;<br/>\r\n- Auto grow shrink script supports to grow/shrink compute nodes created with VM scaleset;<br/>\r\n- Fix the issue that sometimes auto grow shrink script doesn\u2019t use certificate for Azure authentication even when the certificate is configured;<br/>\r\n- Fix the issue that Export-HpcConfiguration.ps1 exports the built-in template \u201cLinuxNode Template\u201d which shall not be exported; <br/>\r\n- Support excludeNodeGroups property in built-in auto grow shrink, user can specify the node group in which he want those nodes to be excluded from auto grow shrink logic;<br/>\r\n-  Add option to disable the node from syncing hpc cluster admin to local administrator group, to do this, you need to add following value on the target node under registry HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\HPC <br/>\r\n&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp Name: DisableSyncWithAdminGroup <br/>\r\n&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp Type: REG_DWORD <br/>\r\n&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp Data: 1 <br/>\r\n<br/>\r\n<b>GUI Improvements</b><br/>\r\n- Show total core in use, running job, running tasks in heatmap view status bar when no node selected;<br/>\r\n- Now you can copy \u201callocated nodes\u201d in job/task detail page;<br/>\r\n- <b>Custom properties page</b> - In the Job dialog, you can now view and edit a job\u2019s custom properties. And if the value of the property is a link, the link is displayed on the page and can be clicked by the user. If you would like a file location to be clickable as well, use the format file:///&lt;location&gt;, for example, file:///c:/users<br/>\r\n- <b>Substitution of mount point</b>  - When a task is executed on a Linux node, the user usually can\u2019t open the working directory. Now within the job management UI you can substitute the mount point by specifying the job custom properties <b>linuxMountPoint</b> and <b>windowsMountPoint</b> so that the user can access the folder as well. For example, you can create a job with the following settings:<br/>\r\n&nbsp&nbsp&nbsp&nbsp&nbsp&nbspCustom Property: linuxMountPoint = /gpfs/Production<br/>\r\n&nbsp&nbsp&nbsp&nbsp&nbsp&nbspCustom Property: windowsMountPoint = Z:\\Production<br/>\r\n&nbsp&nbsp&nbsp&nbsp&nbsp&nbspTask Working Directory: /gpfs/Production/myjob<br/>\r\n   Then when you view the job from GUI, the working directory value in the Job dialog > View Tasks page > Details tab will be z:\\production\\myjob. And if you previously mounted the /gpfs to your local Z: drive, you will be able to view the job output file.<br/>\r\n- Set subscribed information for node - The Administrator can set node subscribed cores or sockets from the GUI. Select offline nodes and perform the Edit Properties action;<br/>\r\n- No copy job \u2013 If you specify the job custom property <b>noGUICopy</b> as true, the Copy action on the GUI will be disabled;<br/>\r\n- Improve HPC job manager heatmap performance issue when there are more than 1000 nodes;<br/>\r\n- Support to copy multiple SOA jobs on SOA job view with the correct format;<br/>\r\n<br/>\r\n<b>REST API and WebComponent fix</b><br/>\r\n- Add new REST API Info/DateTimeFormat to query DataTime format info on HPC Pack REST server so that the client side can do DataTime parsing with the correct format;<br/>\r\n- Improved job searching in HPC Web Portal; Now if you want to get all jobs with name contains \u201cMyJobName\u201d you need to specify \u201c%MyJobName\u201d in search box;<br/>\r\n- Add new odata filter parameters \u201cTaskStates\u201d, \u201cTaskIds\u201d, and \u201cTaskInstanceIds\u201d to the REST API GetTaskList;<br/>\r\n<br/>\r\nNote: This QFE had been refreshed in early June 2017 to address patching issues when your existing cluster is using remote DB with customized scheduler DB name;<br/>\r\n<br/>\r\n<b>This QFE will supercede QFE 3134307, QFE 3147178 and QFE 3161422 released earlier</b><br/>\r\n<br/>\r\n<b>Known issue:</b><br/>\r\nInstallation of the upgrade package KB318996\u00a0does not support SQL Server 2008 R2 or earlier.<br/>\r\n\r\n",
    "detailsSection_kbArticles": {
        "link": "http://support.microsoft.com/kb/",
        "name": ""
    },
    "detailsSection_securityBulletins": {
        "link": "http://technet.microsoft.com/en-us/security/Bulletin/",
        "name": ""
    },
    "detailsSection_file_version": "4.05.5161",
    "detailsSection_file_name": [
        "KB3189996-x64.exe",
        "KB3189996-x86.exe",
        "HpcWebComponents.msi"
    ],
    "detailsSection_file_size": [
        "62.5 MB",
        "62.5 MB",
        "1.6 MB"
    ],
    "detailsSection_file_date": "7/15/2024",
    "systemRequirementsSection": "HPC Pack 2012 R2 Update 3 (build 4.5.5079.0) with or without QFE 3134307/3147178/3161422 (build 4.5.5094.0/4.5.5102.0/4.5.5111) installed.",
    "systemRequirementsSection_supportedOS": [
        "Windows 7",
        "Windows 8",
        "Windows 8.1",
        "Windows Server 2012",
        "Windows Server 2012 R2"
    ],
    "installInstructionSection": "This update needs to be run on all head nodes, broker nodes, workstation nodes, compute nodes and clients<br/>\r\n<br/>\r\nBefore applying the update, please check if HPC Pack 2012 R2 Update 3 is installed. The version number (in HPC Cluster Manager, click <b>Help</b>><b>About</b>) should be 4.5.5079.0 (Or 4.5.5094.0/4.5.5102.0/4.5.5111 if early QFE is installed). Please take all nodes offline and ensure all active jobs finished or canceled. If there are Azure nodes, please make sure they are stopped before applying this patch. After all active operations on the cluster have stopped, please back up the head node (or head nodes) and all HPC databases by using a backup method of your choice.<br/>\r\n<br/>\r\n<b>Important:</b><br/>\r\nThe upgrade package KB318996 does not support uninstallation. After you upgrade, if you want to restore to HPC Pack 2012 R2 Update 3, you must completely uninstall the HPC Pack 2012 R2 Update 3 features from the head node computer and the other computers in your cluster, and reinstall HPC Pack 2012 R2 Update 3 and restore the data in the HPC databases.<br/>\r\n<br/>\r\n<b>Applying the update</b><br/>\r\n<br/>\r\nTo start the download, click the Download button next to the appropriate file (KB3189996-x64.exe for the 64-bit version, KB3189996-x86.exe for the 32-bit version) and then:<br/>\r\n<br/>\r\n1. Click Save to copy the download to your computer.<br/>\r\n2. Close any open HPC Cluster Manager or HPC Job Manager windows.<br/>\r\nNote: Any open instances of HPC Cluster Manager or HPC Job Manager may unexpectedly quit or show an error message during the update process if left open. This does not affect installation of the update.<br/>\r\n3. Run the download on the head node using an administrator account, and reboot the head node.<br/>\r\nNote: If you have high availability head nodes, run the fix on the active node, move the node to passive, and after failover occurs run the fix on the new active node. Do this for all head nodes in the cluster.<br/>\r\n4. Log on interactively, or use clusrun to deploy the fix to the compute nodes, broker nodes, unmanaged server nodes, and workstation nodes.\r\n<br/>\r\nTo use clusrun to patch the QFE on the compute nodes, broker nodes, unmanaged server nodes, and workstation nodes:<br/>\r\n&nbsp&nbsp&nbsp&nbsp&nbsp&nbspa. Copy the appropriate version of the update to a shared folder such as \\\\headnodename\\HPCUpdates .<br/>\r\n&nbsp&nbsp&nbsp&nbsp&nbsp&nbspb. Open an elevated command prompt window and type the appropriate clusrun command for the operating system of the patch, e.g.:<br/>\r\n&nbsp&nbsp&nbsp&nbsp&nbsp&nbspclusrun /nodegroup:ComputeNodes \\\\headnodname\\HPCUpdates\\KB3189996-x64.exe -unattend -SystemReboot<br/>\r\n&nbsp&nbsp&nbsp&nbsp&nbsp&nbspclusrun /nodegroup:BrokerNodes \\\\headnodname\\HPCUpdates\\KB3189996-x64.exe -unattend -SystemReboot<br/>\r\n<b>Note</b>: HPC Pack updates, other than Service Packs, do not get automatically applied when you add a new node to the cluster or re-image an existing node. You must either manually/clusrun apply the update after adding or reimaging a node or modify your node template to include a line to install the appropriate updates from a file share on your head node.<br/>\r\n<b>Note</b>: If the cluster administrator doesn\u2019t have administrative privileges on workstation nodes and unmanaged server node, the clusrun utility may not be able to apply the update. In these cases the update should be performed by the administrator of the workstation and unmanaged servers.<br/>\r\n&nbsp&nbsp&nbsp&nbsp&nbsp&nbspc .To update workstation nodes and unmanaged server nodes you may need to reboot.<br/>\r\n<br/>\r\n5. To update computers that run HPC Pack client applications apply the following actions:<br/>\r\n&nbsp&nbsp&nbsp&nbsp&nbsp&nbspa. Stop any HPC client applications including HPC Job Manager and HPC Cluster Manager<br/>\r\n&nbsp&nbsp&nbsp&nbsp&nbsp&nbspb. Run the update executable<br/>\r\n&nbsp&nbsp&nbsp&nbsp&nbsp&nbspc. Reboot your client computer<br/>\r\n<br/>\r\n6. You can run get-hpcpatchstatus.ps1 on headnode/computenodes/client under %CCP_HOME%bin to check the patch status. And the client version and server version will be 4.05.5158.0<br/>\r\n<br/>\r\n7. To update on premise Linux compute nodes:<br/>\r\n&nbsp&nbsp&nbsp&nbsp&nbsp&nbspa. Set up a file share to share update binaries from head node to Linux compute nodes , for see Get started with on-premises Linux compute nodes).Here we suppose we have established an SMB share C:\\SmbShare on the head node as \\\\headnodename\\SmbShare, and mount it on all the Linux compute nodes with path /smbshare.<br/>\r\n&nbsp&nbsp&nbsp&nbsp&nbsp&nbspb. Find the on-premises Linux compute node installation binaries in the following folder: %CCP_DATA%InstallShare\\LinuxNodeAgent. Then, copy the binaries hpcnodeagent.tar.gz and setup.py into \\\\headnodename\\SmbShare in the head node, and check that the files can be seen in the path /smbshare from the Linux compute nodes.<br/>\r\n&nbsp&nbsp&nbsp&nbsp&nbsp&nbspc. Run update command on the Linux compute nodes in the /smbshare with HPC 2012 R2 Update 3 nodemanger (version 1.7.11.0), e.g.:<br/>\r\n&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsppython setup.py \u2013update<br/>\r\n&nbsp&nbsp&nbsp&nbsp&nbsp&nbspd. (Optional) You can check whether the Linux compute nodes have been updated successfully by using command \u201c/opt/hpcnodemanager/nodemanager -v\u201d on the Linux compute nodes. The updated nodemanager version is 1.7.11.0.<br/>\r\n<br/>",
    "relatedResourcesSection": [
        {
            "title": "hpc Pack QFE KB3161422",
            "url": "https://www.microsoft.com/en-us/download/details.aspx?id=52983"
        },
        {
            "title": "hpc Pack QFE KB3147178 ",
            "url": "https://www.microsoft.com/en-us/download/details.aspx?id=51662"
        },
        {
            "title": "hpc Pack QFE KB3134307",
            "url": "https://www.microsoft.com/en-us/download/details.aspx?id=50809"
        }
    ],
    "locale": "en-us",
    "detailsId": "54772",
    "downloadPreload": true
}